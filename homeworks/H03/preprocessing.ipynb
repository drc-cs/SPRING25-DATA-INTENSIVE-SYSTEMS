{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bff81fb6",
   "metadata": {},
   "source": [
    "# H.03 | Creating an ML-Ready Dataset\n",
    "\n",
    "The penguins dataset is a popular practice dataset for classification tasks. It contains information about different species of penguins, including their physical characteristics and the island they were found on. In this notebook, we will create a machine learning-ready dataset from the penguins dataset. I have made some changes to the original penguins dataset to make our preprocessing more interesting, but the overall structure is the same as can be found online:\n",
    "\n",
    "- bill_length_mm: Bill length (mm) of the penguin.\n",
    "- bill_depth_mm: Bill depth (mm) of the penguin.\n",
    "- flipper_length_mm: Flipper length (mm) of the penguin.\n",
    "- body_mass_g: Body mass (g) of the penguin.\n",
    "- species: Species of the penguin (Adelie, Chinstrap, Gentoo).\n",
    "- island: Island where the penguin was found (Biscoe, Dream).\n",
    "\n",
    "NOTE: This is a cute example, but the principles of data preprocessing are the same for any dataset or domain. This notebook will get you some exposure to numpy and pandas, which are the most common libraries for data manipulation in Python. You will also get some exposure to scikit-learn, which is the most common library for machine learning in Python.\n",
    "\n",
    "For H.03, we are introducing python scripts which are a great way to organize your code. You will write your functions in the `preprocessing.py` file and then import them into this notebook. This is a good practice for larger projects, as it keeps your code organized and makes it easier to maintain. When you submit your code, you will only need to submit the notebook `preprocessing.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8650c14",
   "metadata": {},
   "source": [
    "## Download Dataset\n",
    "\n",
    "There is a csv file hosted on GCP Cloud Storage that contains the penguins dataset. Run the following cell to load the dataset into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae2328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Manage imports.\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "df = pd.read_csv(\"https://storage.googleapis.com/mbai-data/train_dataset.csv\")\n",
    "NUMERICAL_COLUMNS = [ \"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9f5692",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "The first step in any ML or AI project is to perform Exploratory Data Analysis (EDA). This involves examining the dataset to understand its structure, identify any missing values, and visualize the data. In this case, we will use the `pandas` library to load and explore the dataset and plotly to visualize the data.\n",
    "\n",
    "`df.head()` will display the first 5 rows of the dataset, allowing us to see the column names and the first few entries. This is useful for getting a quick overview of the data.\n",
    "\n",
    "`df.describe()` will provide a summary of the dataset, including count, mean, standard deviation, min, max, and quartiles for each numerical column. This is useful for understanding the distribution and range of values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb185be",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8628c92",
   "metadata": {},
   "source": [
    "Looks like we have a dataset with 4 numerical columns and 2 categorical columns. Let's plot the distribution of the numerical columns to see their distributions, since this will have an impact on the methods we choose later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deb9c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting import plot_2x2_histograms\n",
    "\n",
    "fig = plot_2x2_histograms(df, NUMERICAL_COLUMNS)\n",
    "fig.update_layout(title_text=\"Penguin Measurements\", showlegend=False, template = \"plotly_white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2ba79f",
   "metadata": {},
   "source": [
    "These look *approximately* normally distributed, so we can use methods that assume normality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556d1049",
   "metadata": {},
   "source": [
    "## Identify and Fill Missing Values\n",
    "\n",
    "Looking at the output from `df.head()`, we can immediately see that there are some missing values in the dataset. `df.info()` provides a summary of the dataset, including the number of non-null values in each column. This can help identify columns with missing values (and their counts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6914f171",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "display(Markdown(\"------\"))\n",
    "display(df.info())\n",
    "display(Markdown(\"------\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49b948a",
   "metadata": {},
   "source": [
    "We can see that all of the numerical columns have **35 missing values**. Assume for the purposes of this homework that these values are missing at random. Let's use an RandomForest imputation to fill in these missing values. In `preprocessing.py`, you will see a function called impute_numerical_values that you should fill out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4757530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import impute_numerical_values\n",
    "\n",
    "df[NUMERICAL_COLUMNS] = impute_numerical_values(df[NUMERICAL_COLUMNS].to_numpy())\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eb0e7c",
   "metadata": {},
   "source": [
    "That's much better! Now we can see that the missing values have been replaced with estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf6ea9f",
   "metadata": {},
   "source": [
    "## Scale Numerical Data\n",
    "\n",
    "There are several ways to scale data. In class, we covered standardization and min-max scaling and the importance of scaling in your machine learning models. In this case, we will implement both using Numpy. We will ultimately use standard scaling for all numerical columns.\n",
    "\n",
    "Please note: This is a good opportunity to learn some basic functionality in numpy (`x.mean()`, `x.std()`, `x.min()`, `x.max()`). Numpy is a fundamental package for scientific computing in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a81baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import standard_scale_with_numpy, minmax_scale_with_numpy\n",
    "\n",
    "standard_numpy = df.copy()\n",
    "minmax_numpy = df.copy()\n",
    "\n",
    "for feature in NUMERICAL_COLUMNS:\n",
    "    standard_numpy[feature] = standard_scale_with_numpy(df[feature])\n",
    "    minmax_numpy[feature] = minmax_scale_with_numpy(df[feature])\n",
    "\n",
    "display(Markdown(\"------\"))\n",
    "display(Markdown(\"#### Original Data\"))\n",
    "display(df.head())\n",
    "display(Markdown(\"#### Standard Scaled Data\"))\n",
    "display(standard_numpy.head())\n",
    "display(Markdown(\"#### MinMax Scaled Data\"))\n",
    "display(minmax_numpy.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e16a62",
   "metadata": {},
   "source": [
    "## Encode Species Variable\n",
    "\n",
    "The species variable is categorical, so we need to encode it as a numerical variable. We will use one-hot encoding to create binary columns for each species. This is a common technique for handling categorical variables in machine learning. We covered an example using colors in class.\n",
    "\n",
    "Pandas has a built-in function for one-hot encoding, `pd.get_dummies()`, which will create a new column for each unique value in the species column. This will effectively perform one-hot encoding. We will also drop the original species column after encoding, since we no longer need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dbf72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import generate_one_hot_encoding\n",
    "\n",
    "df = standard_numpy.copy()\n",
    "df = generate_one_hot_encoding(df)\n",
    "\n",
    "display(Markdown(\"#### One Hot Encoded Data\"))\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a2b32e",
   "metadata": {},
   "source": [
    "## Create Target Variable\n",
    "\n",
    "Creating a target variable is an important step in preparing your dataset for machine learning. In this case, we will create a target variable called `island` that indicates whether the island is `Biscoe` or `Dream`. This represents a binary classification problem, where we want to predict the island based on the other features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d00494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import binarize_islands\n",
    "\n",
    "df['island'] = binarize_islands(df['island'])\n",
    "\n",
    "display(Markdown(\"#### Binarized Islands\"))\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574f6193",
   "metadata": {},
   "source": [
    "## Reorder Columns for Convention\n",
    "\n",
    "By convention, we want to have the target variable as the last column in our dataset. This is a common practice in machine learning, as it makes it easier to separate the features from the target variable when training a model. We will reorder the columns in the dataframe to place the target variable at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d916f93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import reorder_columns\n",
    "\n",
    "df = reorder_columns(df)\n",
    "display(Markdown(\"#### Reordered Columns\"))\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334c128b",
   "metadata": {},
   "source": [
    "## Submit\n",
    "\n",
    "Make sure you save your `preprocessing.py` file before submitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49743323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from submit import send_notebook\n",
    "\n",
    "response = send_notebook(\"./preprocessing.py\")\n",
    "print(response[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e41edb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mbai-dis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
